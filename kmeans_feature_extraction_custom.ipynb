{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\.conda\\envs\\RS\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\user\\.conda\\envs\\RS\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1440: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import mapping\n",
    "import rasterio\n",
    "from rasterio.features import geometry_mask\n",
    "import numpy as np\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "import h5py\n",
    "import cv2\n",
    "from skimage.color import rgb2lab, rgb2gray\n",
    "from skimage.filters import roberts, sobel\n",
    "from skimage.filters.rank import entropy\n",
    "from skimage.morphology import disk\n",
    "from skimage.transform import resize\n",
    "from skimage.util import img_as_ubyte\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Define a transformation to apply to image patches\n",
    "transform = T.Compose([\n",
    "    T.Resize((224, 224)),\n",
    "    T.ToTensor()\n",
    "])\n",
    "\n",
    "def extract_polygon_patch(polygon, image_data, transform):\n",
    "    \"\"\"Extract image patch within a polygon\"\"\"\n",
    "    mask = geometry_mask([mapping(polygon)], transform=transform, invert=True, out_shape=(image_data.shape[1], image_data.shape[2]))\n",
    "    masked_data = np.zeros_like(image_data[:3])\n",
    "    for i in range(3):\n",
    "        masked_data[i] = image_data[i] * mask\n",
    "    bounds = polygon.bounds\n",
    "    window = rasterio.windows.from_bounds(*bounds, transform=transform)\n",
    "    row_off = int(window.row_off)\n",
    "    col_off = int(window.col_off)\n",
    "    height = int(window.height)\n",
    "    width = int(window.width)\n",
    "    patch = masked_data[:, row_off:row_off + height, col_off:col_off + width]\n",
    "    patch = np.moveaxis(patch, 0, -1)\n",
    "    patch = Image.fromarray(patch.astype(np.uint8))\n",
    "    return patch\n",
    "\n",
    "def process_shapefile(shapefile_path, tif_file_path, transform):\n",
    "    \"\"\"Process shapefile to extract patches, features, and perform clustering\"\"\"\n",
    "    polygons = gpd.read_file(shapefile_path)\n",
    "    patches_and_labels = []\n",
    "    with rasterio.open(tif_file_path) as src:\n",
    "        image_data = src.read()\n",
    "\n",
    "    for idx, row in polygons.iterrows():\n",
    "        polygon = row.geometry\n",
    "        label = row['sp']\n",
    "        patch = extract_polygon_patch(polygon, image_data, src.transform)\n",
    "\n",
    "        # Transform the patch and append to the container\n",
    "        transformed_patch = transform(patch)\n",
    "        patches_and_labels.append((transformed_patch, label))\n",
    "        \n",
    "        # # Display the resized patch\n",
    "        # resized_patch = transformed_patch.permute(1, 2, 0)  # Change from (C, H, W) to (H, W, C)\n",
    "        # resized_patch = resized_patch.clamp(0, 1)  # Ensure the values are within [0, 1]\n",
    "        # plt.imshow(resized_patch.numpy())\n",
    "        # plt.title(f'Polygon Label: {label}')\n",
    "        # plt.show()\n",
    "\n",
    "    return patches_and_labels\n",
    "\n",
    "def extract_custom_features(img):\n",
    "    # Color features\n",
    "    LAB_img = rgb2lab(img)\n",
    "    A_img = LAB_img[:,:,1]\n",
    "    A_feat = A_img.mean()\n",
    "    \n",
    "    B_img = LAB_img[:,:,2]\n",
    "    B_feat = B_img.mean()\n",
    "    \n",
    "    # Textural features based on the gray image\n",
    "    gray_img = rgb2gray(img) \n",
    "    gray_img = resize(gray_img, (256,256))  # Resize to smaller size\n",
    "    gray_img = img_as_ubyte(gray_img)\n",
    "   \n",
    "    # Entropy\n",
    "    entropy_img = entropy(gray_img, disk(3))\n",
    "    entropy_mean = entropy_img.mean()\n",
    "    entropy_std = entropy_img.std()\n",
    "    \n",
    "    roberts_img = roberts(gray_img)\n",
    "    roberts_mean = roberts_img.mean()\n",
    "\n",
    "    sobel_img = sobel(gray_img)\n",
    "    sobel_mean = sobel_img.mean()\n",
    "    \n",
    "    # Gabor filters\n",
    "    kernel1 = cv2.getGaborKernel((9, 9), 3, np.pi/4, np.pi, 0.5, 0, ktype=cv2.CV_32F)    \n",
    "    gabor1 = (cv2.filter2D(gray_img, cv2.CV_8UC3, kernel1)).mean()\n",
    "    \n",
    "    kernel2 = cv2.getGaborKernel((9, 9), 3, np.pi/2, np.pi/4, 0.9, 0, ktype=cv2.CV_32F)    \n",
    "    gabor2 = (cv2.filter2D(gray_img, cv2.CV_8UC3, kernel2)).mean()\n",
    "\n",
    "    kernel3 = cv2.getGaborKernel((9, 9), 5, np.pi/2, np.pi/2, 0.1, 0, ktype=cv2.CV_32F)    \n",
    "    gabor3 = (cv2.filter2D(gray_img, cv2.CV_8UC3, kernel3)).mean()\n",
    "\n",
    "    custom_features = np.array([A_feat, B_feat, entropy_mean, entropy_std, roberts_mean, \n",
    "                                sobel_mean, gabor1, gabor2, gabor3])\n",
    "    \n",
    "    return custom_features\n",
    "\n",
    "def extract_features(patches_and_labels):\n",
    "    \"\"\"Extract custom features\"\"\"\n",
    "    features = []\n",
    "    labels = []\n",
    "    for patch, label in patches_and_labels:\n",
    "        patch_np = patch.permute(1, 2, 0).numpy() * 255  # Convert to numpy array and scale back to 0-255\n",
    "        patch_np = patch_np.astype(np.uint8)\n",
    "        feature = extract_custom_features(patch_np)\n",
    "        features.append(feature)\n",
    "        labels.append(label)\n",
    "    \n",
    "    return features, labels\n",
    "\n",
    "def perform_kmeans_clustering(features_array, n_clusters=6):\n",
    "    \"\"\"Perform K-means clustering on extracted features\"\"\"\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "    return kmeans.fit_predict(features_array)\n",
    "\n",
    "def process_shapefile_for_clustering(shapefile_path, tif_file_path, output_shapefile_path, transform):\n",
    "    \"\"\"Process shapefile to extract patches, features, and perform clustering\"\"\"\n",
    "    patches_and_labels = process_shapefile(shapefile_path, tif_file_path, transform)\n",
    "    features, labels = extract_features(patches_and_labels)\n",
    "\n",
    "    features_array = np.array(features)\n",
    "    cluster_labels = perform_kmeans_clustering(features_array)\n",
    "    \n",
    "    polygons = gpd.read_file(shapefile_path)\n",
    "    polygons['cluster'] = cluster_labels\n",
    "    polygons.to_file(output_shapefile_path)\n",
    "    return polygons\n",
    "\n",
    "# Example usage\n",
    "shapefile_path = 'h:\\\\Yehmh\\\\DNDF\\\\101_1_focus\\\\202404_101_seg_shp_labeled\\\\DNDF101_clip_seg_labeled.shp'\n",
    "tif_file_path = 'h:\\\\Yehmh\\\\DNDF\\\\101_1_focus\\\\DNDF101_clip.tif'\n",
    "output_shapefile_path = 'h:\\\\Yehmh\\\\DNDF\\\\101_1_focus\\\\DNDF_202404_101_1_kmeans_custom.shp'\n",
    "\n",
    "clustered_polygons = process_shapefile_for_clustering(shapefile_path, tif_file_path, output_shapefile_path, transform)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
